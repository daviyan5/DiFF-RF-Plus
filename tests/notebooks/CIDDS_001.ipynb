{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 99\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATASET_NAME = \"CIDDS-001\"\n",
    "# Definindo o BASE_DIR\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Caminho para a pasta de dados\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, 'data_raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'data_preprocessed')\n",
    "\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando o dataset\n",
    "TEMP_DIR = '/tmp'\n",
    "ZIP_FILE_PATH = os.path.join(TEMP_DIR, 'CIDDS-001.zip')\n",
    "if not os.path.exists(ZIP_FILE_PATH):\n",
    "    print(\"Baixando o dataset...\")\n",
    "    !wget 'https://www.hs-coburg.de/wp-content/uploads/2024/11/CIDDS-001.zip' -O {ZIP_FILE_PATH}\n",
    "\n",
    "TMP_FOLDER = os.path.join(RAW_DATA_DIR, 'tmp')\n",
    "if not os.path.exists(os.path.join(RAW_DATA_DIR, DATASET_NAME)):\n",
    "    print(\"Descompactando o dataset...\")\n",
    "    !unzip {ZIP_FILE_PATH} -d {TMP_FOLDER}\n",
    "\n",
    "    from_dir = TMP_FOLDER\n",
    "    to_dir = os.path.join(RAW_DATA_DIR, DATASET_NAME)\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    \n",
    "    for root, dirs, files in os.walk(from_dir):\n",
    "        for file_name in files:\n",
    "            src_path = os.path.join(root, file_name)\n",
    "            dest_path = os.path.join(to_dir, file_name)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "    shutil.rmtree(TMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(os.path.join(RAW_DATA_DIR, DATASET_NAME)):\n",
    "    for file_name in files:\n",
    "        if (file_name.endswith('.log') or file_name.endswith('.conf') or \n",
    "            file_name.endswith('.pdf') or file_name.startswith('attack_logs')):\n",
    "            os.remove(os.path.join(root, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52109/2664548688.py:4: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_aux = pd.read_csv(os.path.join(DATASET_DIR, file))\n",
      "/tmp/ipykernel_52109/2664548688.py:4: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_aux = pd.read_csv(os.path.join(DATASET_DIR, file))\n",
      "/tmp/ipykernel_52109/2664548688.py:4: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_aux = pd.read_csv(os.path.join(DATASET_DIR, file))\n",
      "/tmp/ipykernel_52109/2664548688.py:4: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_aux = pd.read_csv(os.path.join(DATASET_DIR, file))\n",
      "/tmp/ipykernel_52109/2664548688.py:4: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_aux = pd.read_csv(os.path.join(DATASET_DIR, file))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Date first seen', 'Duration', 'Proto', 'Src IP Addr', 'Src Pt',\n",
       "       'Dst IP Addr', 'Dst Pt', 'Packets', 'Bytes', 'Flows', 'Flags', 'Tos',\n",
       "       'class', 'attackType', 'attackID', 'attackDescription'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_DIR = os.path.join(RAW_DATA_DIR, DATASET_NAME)\n",
    "df_list = []\n",
    "for file in os.listdir(DATASET_DIR):\n",
    "    df_aux = pd.read_csv(os.path.join(DATASET_DIR, file))\n",
    "    df_list.append(df_aux)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo colunas indesejadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\n",
    "    \"Date first seen\",\n",
    "    \"Src IP Addr\",\n",
    "    \"Dst IP Addr\",\n",
    "    'Src Pt',\n",
    "    'Dst Pt'\n",
    "]\n",
    "df.drop(columns=columns_to_remove, inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo dados nulos e duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Duration             float64\n",
       "Proto                 object\n",
       "Packets                int64\n",
       "Bytes                 object\n",
       "Flows                  int64\n",
       "Flags                 object\n",
       "Tos                    int64\n",
       "class                 object\n",
       "attackType            object\n",
       "attackID              object\n",
       "attackDescription     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratando as colunas\n",
    "As colunas restantes são:\n",
    "```\n",
    "Duration             float64\n",
    "Proto                 object\n",
    "Packets                int64\n",
    "Bytes                 object\n",
    "Flows                  int64\n",
    "Flags                 object\n",
    "Tos                    int64\n",
    "class                 object\n",
    "attackType            object\n",
    "attackID              object\n",
    "attackDescription     object\n",
    "Bytes Numeric        float64\n",
    "```\n",
    "\n",
    "Tratamento: \n",
    "- Proto: \n",
    "    One Hot Encoding\n",
    "- Bytes\n",
    "    Converter para float\n",
    "- Flags: \n",
    "    Converter para binário + One Hot Encoding com os bits\n",
    "- Tos: \n",
    "    One Hot Encoding\n",
    "- class: \n",
    "    Remover as linhas que não sejam \"normal, suspicious, attacker\"\n",
    "- attackType, attackID, attackDescription: \n",
    "    Após a divisão treino/teste/validação, remover as colunas dos conjuntos de treino e validação. \n",
    "    Remover do conjunto de teste, mas salvar os valores em test_attacks.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Proto: One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"Proto\"], prefix=\"Proto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bytes: Converter para float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bytes(val):\n",
    "    if isinstance(val, str) and 'M' in val:\n",
    "        return float(val.replace('M', '')) * 1024 * 1024\n",
    "    else:\n",
    "        return float(val)\n",
    "\n",
    "df[\"Bytes\"] = df[\"Bytes\"].apply(convert_bytes)\n",
    "df[\"Bytes Numeric\"] = pd.to_numeric(df[\"Bytes\"], errors='coerce')\n",
    "df.drop(columns=[\"Bytes\"], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Flags: Converter para binário + One Hot Encoding com os bits\n",
    "Alguns valores de Flags estão no formato:\n",
    "..X.X. (string com 6 caracteres. Cada letra representa um bit e cada . representa 0)\n",
    "0xNN onde NN é um número hexadecimal.\n",
    "\n",
    "- Converter para um binário de 8 bits\n",
    "- Criar uma nova coluna para cada bit\n",
    "- Setar o valor 1 ou 0 de acordo com o valor do bit na coluna original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_flag_to_binary(flag):\n",
    "    flag = flag.strip()\n",
    "    if flag.startswith(\"0x\"):\n",
    "        try:\n",
    "            num = int(flag, 16)\n",
    "            return format(num, '08b')\n",
    "        except:\n",
    "            return \"00000000\"\n",
    "    else:\n",
    "        # Assume flag is a string like \"..X.X.\" (6 characters)\n",
    "        if len(flag) == 6:\n",
    "            bits = ''.join('1' if ch != '.' else '0' for ch in flag)\n",
    "            return \"00\" + bits  # pad with two zeros to form 8 bits\n",
    "        elif len(flag) == 8:\n",
    "            return ''.join('1' if ch != '.' else '0' for ch in flag)\n",
    "        else:\n",
    "            return ''.join('1' if ch != '.' else '0' for ch in flag)\n",
    "\n",
    "df[\"flag_binary\"] = df[\"Flags\"].apply(convert_flag_to_binary)\n",
    "for i in range(8):\n",
    "    df[f'flag_bit_{i}'] = df[\"flag_binary\"].str[i].astype(int)\n",
    "df.drop(columns=[\"Flags\", \"flag_binary\"], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tos: One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"Tos\"], prefix=\"Tos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. class: Remover as linhas que não sejam \"normal, suspicious, attacker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"class\"].isin([\"normal\", \"suspicious\", \"attacker\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    4222389\n",
       "1     197047\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conver normal to 0 and suspicious/attacker to 1\n",
    "df[\"class\"] = df[\"class\"].replace({\"normal\": 0, \"suspicious\": 1, \"attacker\": 1}).infer_objects(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 duplicate rows.\n"
     ]
    }
   ],
   "source": [
    "old_len = len(df)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "new_len = len(df)\n",
    "print(f\"Removed {old_len - new_len} duplicate rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Duration             float64\n",
       "Packets                int64\n",
       "Flows                  int64\n",
       "class                  int64\n",
       "attackType            object\n",
       "attackID              object\n",
       "attackDescription     object\n",
       "Proto_GRE               bool\n",
       "Proto_ICMP              bool\n",
       "Proto_IGMP              bool\n",
       "Proto_TCP               bool\n",
       "Proto_UDP               bool\n",
       "Bytes Numeric        float64\n",
       "flag_bit_0             int64\n",
       "flag_bit_1             int64\n",
       "flag_bit_2             int64\n",
       "flag_bit_3             int64\n",
       "flag_bit_4             int64\n",
       "flag_bit_5             int64\n",
       "flag_bit_6             int64\n",
       "flag_bit_7             int64\n",
       "Tos_0                   bool\n",
       "Tos_12                  bool\n",
       "Tos_16                  bool\n",
       "Tos_32                  bool\n",
       "Tos_192                 bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo colunas com alta correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tos_0', 'Proto_TCP  ', 'Proto_UDP  ']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_highly_correlated_features(correlation_matrix, threshold):\n",
    "    correlated_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                pair = (correlation_matrix.columns[i], correlation_matrix.columns[j])\n",
    "                coefficient = correlation_matrix.iloc[i, j]\n",
    "                correlated_pairs.append((pair, coefficient))\n",
    "    return sorted(correlated_pairs, key= lambda pair: pair[1], reverse=True)\n",
    "\n",
    "df_without_attacks = df.drop([\"attackType\", \"attackID\", \"attackDescription\"], axis=1, errors='ignore')\n",
    "df_without_attacks = df_without_attacks[df_without_attacks[\"class\"] == 0].copy()\n",
    "corr_matrix = df_without_attacks.corr().abs()\n",
    "correlation_list = get_highly_correlated_features(corr_matrix, 0.95)\n",
    "\n",
    "# Drop high correlated features in correlation list\n",
    "\n",
    "f2drop = []\n",
    "for feature_pair, _ in correlation_list:\n",
    "    if feature_pair[0] not in f2drop and feature_pair[1] not in f2drop:\n",
    "        f2drop.append(feature_pair[1])\n",
    "\n",
    "f2drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=f2drop, inplace=True, errors='ignore')\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df[\"class\"] == 0].sample(frac=0.6, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "df_val_test = df.drop(df_train.index).reset_index(drop=True)\n",
    "\n",
    "df_val, df_test = train_test_split(df_val_test, test_size=0.65, stratify=df_val_test['class'], random_state=RANDOM_SEED)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "drop_cols = ['class', 'attackType', 'attackID', 'attackDescription']\n",
    "\n",
    "X_train = df_train.drop(columns=drop_cols)\n",
    "X_val = df_val.drop(columns=drop_cols)\n",
    "y_train = df_train['class']   # should be all 0\n",
    "y_val = df_val['class']\n",
    "\n",
    "test_attacks = df_test[['attackType', 'attackID', 'attackDescription']].copy()\n",
    "os.makedirs(os.path.join(PROCESSED_DATA_DIR, DATASET_NAME), exist_ok=True)\n",
    "test_attacks.to_csv(os.path.join(PROCESSED_DATA_DIR, DATASET_NAME, 'test_attacks.csv'), index=False)\n",
    "X_test = df_test.drop(columns=drop_cols)\n",
    "y_test = df_test['class']\n",
    "\n",
    "del df_train, df_val, df_test, df_val_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaler = std_scaler.fit(X_train)\n",
    "\n",
    "norm_X_train = std_scaler.transform(X_train)\n",
    "norm_X_val = std_scaler.transform(X_val)\n",
    "norm_X_test = std_scaler.transform(X_test)\n",
    "\n",
    "# Salvando os dados processados em \"data_preprocessed\"\n",
    "X_train = pd.DataFrame(norm_X_train, columns=X_train.columns)\n",
    "X_val = pd.DataFrame(norm_X_val, columns=X_val.columns)\n",
    "X_test = pd.DataFrame(norm_X_test, columns=X_test.columns)\n",
    "\n",
    "# Salvando os arquivos processados\n",
    "\n",
    "RESULT_DIR = os.path.join(PROCESSED_DATA_DIR, DATASET_NAME)\n",
    "X_train.to_csv(os.path.join(RESULT_DIR, 'X_train.csv'), index=False)\n",
    "X_val.to_csv(os.path.join(RESULT_DIR, 'X_val.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(RESULT_DIR, 'X_test.csv'), index=False)\n",
    "y_val.to_csv(os.path.join(RESULT_DIR, 'y_val.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(RESULT_DIR, 'y_test.csv'), index=False)\n",
    "\n",
    "del X_train, X_val, X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
