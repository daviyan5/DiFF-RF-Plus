{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 99\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATASET_NAME = \"Kitsune\"\n",
    "# Definindo o BASE_DIR\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Caminho para a pasta de dados\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, 'data_raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'data_preprocessed')\n",
    "\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descompactando o dataset...\n",
      "Archive:  /tmp/Kitsune.zip\n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/ARP MitM/ARP_MitM_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/ARP MitM/ARP_MitM_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/ARP MitM/ARP_MitM_pcap.pcapng  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Active Wiretap/Active_Wiretap_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Active Wiretap/Active_Wiretap_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Active Wiretap/Active_Wiretap_pcap.pcapng  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Fuzzing/Fuzzing_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Fuzzing/Fuzzing_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Fuzzing/Fuzzing_pcap.pcapng  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Mirai Botnet/Mirai_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Mirai Botnet/Mirai_pcap.pcap  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Mirai Botnet/mirai_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/OS Scan/OS_Scan_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/OS Scan/OS_Scan_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/OS Scan/OS_Scan_pcap.pcapng  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SSDP Flood/SSDP_Flood_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SSDP Flood/SSDP_Flood_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SSDP Flood/SSDP_Flood_pcap.pcap  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SSL Renegotiation/SSL_Renegotiation_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SSL Renegotiation/SSL_Renegotiation_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SSL Renegotiation/SSL_Renegotiation_pcap.pcap  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SYN DoS/SYN_DoS_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SYN DoS/SYN_DoS_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/SYN DoS/SYN_DoS_pcap.pcap  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Video Injection/Video_Injection_dataset.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Video Injection/Video_Injection_labels.csv  \n",
      "  inflating: /mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/data/data_raw/tmp/Video Injection/Video_Injection_pcap.pcapng  \n"
     ]
    }
   ],
   "source": [
    "# Baixando o dataset\n",
    "TEMP_DIR = '/tmp'\n",
    "ZIP_FILE_PATH = os.path.join(TEMP_DIR, 'Kitsune.zip')\n",
    "if not os.path.exists(ZIP_FILE_PATH):\n",
    "    print(\"Baixando o dataset...\")\n",
    "    !wget 'https://www.kaggle.com/api/v1/datasets/download/ymirsky/network-attack-dataset-kitsune' -O {ZIP_FILE_PATH}\n",
    "\n",
    "TMP_FOLDER = os.path.join(RAW_DATA_DIR, 'tmp')\n",
    "if not os.path.exists(os.path.join(RAW_DATA_DIR, DATASET_NAME)):\n",
    "    print(\"Descompactando o dataset...\")\n",
    "    !unzip {ZIP_FILE_PATH} -d {TMP_FOLDER}\n",
    "\n",
    "    from_dir = TMP_FOLDER\n",
    "    to_dir = os.path.join(RAW_DATA_DIR, DATASET_NAME)\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    \n",
    "    for root, dirs, files in os.walk(from_dir):\n",
    "        for file_name in files:\n",
    "            src_path = os.path.join(root, file_name)\n",
    "            dest_path = os.path.join(to_dir, file_name)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "    shutil.rmtree(TMP_FOLDER)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os arquivos desse dataset são muito grande. Então, realizaremos alguns processamentos adicionais para remover arquivos desnecessários e diminuir consideravelmente o tamanho do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all .pcap files\n",
    "for root, dirs, files in os.walk(os.path.join(RAW_DATA_DIR, DATASET_NAME)):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.pcapng') or file_name.endswith('.pcap'):\n",
    "            os.remove(os.path.join(root, file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Mirai_dataset.csv and mirai_labels.csv...\n",
      "Processing OS_Scan_dataset.csv and OS_Scan_labels.csv...\n",
      "Processing SSDP_Flood_dataset.csv and SSDP_Flood_labels.csv...\n",
      "Processing SSL_Renegotiation_dataset.csv and SSL_Renegotiation_labels.csv...\n",
      "Processing SYN_DoS_dataset.csv and SYN_DoS_labels.csv...\n",
      "Processing Video_Injection_dataset.csv and Video_Injection_labels.csv...\n"
     ]
    }
   ],
   "source": [
    "for file_name in os.listdir(os.path.join(RAW_DATA_DIR, DATASET_NAME)):\n",
    "    if file_name.endswith('_dataset.csv'):\n",
    "        dataset_file = os.path.join(RAW_DATA_DIR, DATASET_NAME, file_name)\n",
    "        labels_file = dataset_file.replace('_dataset.csv', '_labels.csv')\n",
    "\n",
    "        # Mirai_dataset is wrongly named (mirai_labels.csv)\n",
    "        if file_name.startswith('Mirai_dataset'):\n",
    "            labels_file = os.path.join(RAW_DATA_DIR, DATASET_NAME, 'mirai_labels.csv')\n",
    "            MIRAI_DATASET = True\n",
    "        else:\n",
    "            MIRAI_DATASET = False\n",
    "\n",
    "        if os.path.exists(labels_file):\n",
    "            print(f\"Processing {file_name} and {os.path.basename(labels_file)}...\")\n",
    "\n",
    "            # Read dataset and labels files into pandas dataframes\n",
    "            df_dataset = pd.read_csv(dataset_file, header=None)\n",
    "            df_dataset.columns = [f\"feat{i+1}\" for i in range(df_dataset.shape[1])]\n",
    "\n",
    "            if MIRAI_DATASET:\n",
    "                df_labels = pd.read_csv(labels_file, header=None)\n",
    "                df_labels.columns = [\"Anomaly\"]\n",
    "            else:\n",
    "                df_labels = pd.read_csv(labels_file)\n",
    "                df_labels = df_labels.iloc[:, [1]]\n",
    "                df_labels.columns = [\"Anomaly\"]\n",
    "\n",
    "            # Merge the dataset and labels\n",
    "            df_merged = pd.concat([df_dataset, df_labels], axis=1)\n",
    "\n",
    "            # Calculate factor (2000 / (size of dataset + size of labels))\n",
    "            dataset_size = os.path.getsize(dataset_file)\n",
    "            labels_size = os.path.getsize(labels_file)\n",
    "            total_size = dataset_size + labels_size\n",
    "            factor = (2000 * 1024 * 1024) / total_size\n",
    "            factor = min(factor, 1.0)\n",
    "\n",
    "            # Calculate the number of rows to keep based on the factor\n",
    "            n_rows_to_keep = int(len(df_merged) * factor)\n",
    "            df_merged = df_merged.sample(n=n_rows_to_keep, random_state=RANDOM_SEED)\n",
    "\n",
    "            # Save the merged dataframe as NAME.csv\n",
    "            merged_file_path = os.path.join(RAW_DATA_DIR, DATASET_NAME, file_name.replace('_dataset.csv', '.csv'))\n",
    "            df_merged.to_csv(merged_file_path, index=False)\n",
    "\n",
    "            # Delete the original dataset and labels files\n",
    "            os.remove(dataset_file)\n",
    "            os.remove(labels_file)\n",
    "        else:\n",
    "            print(f\"Labels file {labels_file} not found for {file_name}. Skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(os.path.join(RAW_DATA_DIR, DATASET_NAME)):\n",
    "    if file_name.endswith('.csv'):\n",
    "        DATA_NAME = file_name.replace('.csv', '')\n",
    "        processed_folder = os.path.join(PROCESSED_DATA_DIR, f\"Kitsune_{DATA_NAME}\")\n",
    "        os.makedirs(processed_folder, exist_ok=True)\n",
    "        dataset_file = os.path.join(RAW_DATA_DIR, DATASET_NAME, file_name)\n",
    "        df_dataset = pd.read_csv(dataset_file)\n",
    "        df_dataset = df_dataset.dropna().drop_duplicates()\n",
    "        \n",
    "        # Test/train split (assuming benign samples are labeled as 0 in the \"Anomaly\" column)\n",
    "        df_train = df_dataset.query('Anomaly == 0').sample(frac=0.6, random_state=RANDOM_SEED)\n",
    "        df_val_test = df_dataset.drop(df_train.index)\n",
    "        df_train = df_train.reset_index(drop=True)\n",
    "        df_val_test = df_val_test.reset_index(drop=True)\n",
    "        \n",
    "        X_train = df_train.drop('Anomaly', axis=1)\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            df_val_test.drop('Anomaly', axis=1),\n",
    "            df_val_test['Anomaly'],\n",
    "            test_size=0.65,\n",
    "            stratify=df_val_test['Anomaly'],\n",
    "            random_state=RANDOM_SEED\n",
    "        )\n",
    "        X_val, X_test = X_val.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "        y_val, y_test = y_val.reset_index(drop=True), y_test.reset_index(drop=True)\n",
    "        del df_train, df_val_test\n",
    "        \n",
    "        # Correlation analysis and drop highly correlated features\n",
    "        def get_highly_correlated_features(corr_matrix, threshold):\n",
    "            correlated_pairs = []\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i):\n",
    "                    if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                        pair = (corr_matrix.columns[i], corr_matrix.columns[j])\n",
    "                        coefficient = corr_matrix.iloc[i, j]\n",
    "                        correlated_pairs.append((pair, coefficient))\n",
    "            return sorted(correlated_pairs, key=lambda pair: pair[1], reverse=True)\n",
    "        \n",
    "        corr_matrix = X_train.corr().abs()\n",
    "        correlation_list = get_highly_correlated_features(corr_matrix, 0.95)\n",
    "        f2drop = []\n",
    "        for feature_pair, _ in correlation_list:\n",
    "            if feature_pair[0] not in f2drop and feature_pair[1] not in f2drop:\n",
    "                f2drop.append(feature_pair[1])\n",
    "        X_train = X_train.drop(f2drop, axis=1)\n",
    "        X_val = X_val.drop(f2drop, axis=1)\n",
    "        X_test = X_test.drop(f2drop, axis=1)\n",
    "        \n",
    "        # Normalization\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        std_scaler = StandardScaler().fit(X_train)\n",
    "        norm_X_train = std_scaler.transform(X_train)\n",
    "        norm_X_val = std_scaler.transform(X_val)\n",
    "        norm_X_test = std_scaler.transform(X_test)\n",
    "        X_train = pd.DataFrame(norm_X_train, columns=X_train.columns)\n",
    "        X_val = pd.DataFrame(norm_X_val, columns=X_val.columns)\n",
    "        X_test = pd.DataFrame(norm_X_test, columns=X_test.columns)\n",
    "        \n",
    "        # Save processed files\n",
    "        X_train.to_csv(os.path.join(processed_folder, 'X_train.csv'), index=False)\n",
    "        X_val.to_csv(os.path.join(processed_folder, 'X_val.csv'), index=False)\n",
    "        X_test.to_csv(os.path.join(processed_folder, 'X_test.csv'), index=False)\n",
    "        y_val.to_csv(os.path.join(processed_folder, 'y_val.csv'), index=False)\n",
    "        y_test.to_csv(os.path.join(processed_folder, 'y_test.csv'), index=False)\n",
    "        \n",
    "        del X_train, X_val, X_test, y_val, y_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
