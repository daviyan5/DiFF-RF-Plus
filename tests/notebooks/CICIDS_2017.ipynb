{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 99\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATASET_NAME = \"CICIDS_2017\"\n",
    "# Definindo o BASE_DIR\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Caminho para a pasta de dados\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "RAW_DATA_DIR = os.path.join(DATA_DIR, 'data_raw')\n",
    "PROCESSED_DATA_DIR = os.path.join(DATA_DIR, 'data_preprocessed')\n",
    "\n",
    "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando o dataset\n",
    "TEMP_DIR = '/tmp'\n",
    "ZIP_FILE_PATH = os.path.join(TEMP_DIR, 'CIC_IDS_2017.zip')\n",
    "if not os.path.exists(ZIP_FILE_PATH):\n",
    "    print(\"Baixando o dataset...\")\n",
    "    !wget 'http://205.174.165.80/CICDataset/CIC-IDS-2017/Dataset/CIC-IDS-2017/CSVs/MachineLearningCSV.zip' -O {ZIP_FILE_PATH}\n",
    "\n",
    "TMP_FOLDER = os.path.join(RAW_DATA_DIR, 'tmp')\n",
    "if not os.path.exists(os.path.join(RAW_DATA_DIR, DATASET_NAME)):\n",
    "    print(\"Descompactando o dataset...\")\n",
    "    !unzip {ZIP_FILE_PATH} -d {TMP_FOLDER}\n",
    "\n",
    "    from_dir = TMP_FOLDER\n",
    "    to_dir = os.path.join(RAW_DATA_DIR, DATASET_NAME)\n",
    "    os.makedirs(to_dir, exist_ok=True)\n",
    "    \n",
    "    for root, dirs, files in os.walk(from_dir):\n",
    "        for file_name in files:\n",
    "            src_path = os.path.join(root, file_name)\n",
    "            dest_path = os.path.join(to_dir, file_name)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "    shutil.rmtree(TMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Destination Port', 'Flow Duration', 'Total Fwd Packets',\n",
       "       'Total Backward Packets', 'Total Length of Fwd Packets',\n",
       "       'Total Length of Bwd Packets', 'Fwd Packet Length Max',\n",
       "       'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
       "       'Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       'Bwd Packet Length Min', 'Bwd Packet Length Mean',\n",
       "       'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s',\n",
       "       'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
       "       'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max',\n",
       "       'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std',\n",
       "       'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
       "       'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Length',\n",
       "       'Bwd Header Length', 'Fwd Packets/s', 'Bwd Packets/s',\n",
       "       'Min Packet Length', 'Max Packet Length', 'Packet Length Mean',\n",
       "       'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count',\n",
       "       'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
       "       'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio',\n",
       "       'Average Packet Size', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size',\n",
       "       'Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', 'Fwd Avg Packets/Bulk',\n",
       "       'Fwd Avg Bulk Rate', 'Bwd Avg Bytes/Bulk', 'Bwd Avg Packets/Bulk',\n",
       "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
       "       'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
       "       'Init_Win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
       "       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
       "       'Idle Std', 'Idle Max', 'Idle Min', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_DIR = os.path.join(RAW_DATA_DIR, DATASET_NAME)\n",
    "df_list = []\n",
    "for file in os.listdir(DATASET_DIR):\n",
    "    df_aux = pd.read_csv(os.path.join(DATASET_DIR, file))\n",
    "    df_list.append(df_aux)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho inicial: 2830743, tamanho final 2522362 | Descartadas 308381 duplicadas\n",
      "Tamanho inicial: 2522362, tamanho final 2522009 | Descartados 353 registros com valores NA\n"
     ]
    }
   ],
   "source": [
    "# Descartando duplicadas\n",
    "initial_len = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartadas {initial_len - df.shape[0]} duplicadas')\n",
    "\n",
    "# Descartando registros com valores NaN/Null/NA\n",
    "initial_len = df.shape[0]\n",
    "df = df.dropna()\n",
    "print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartados {initial_len - df.shape[0]} registros com valores NA')\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes = 0\n",
    "for column in df.columns:\n",
    "    if df[column].dtype in ['float64', 'int64']:\n",
    "        max_finite_value = df[np.isfinite(df[column])][column].max()\n",
    "        min_finite_value = df[np.isfinite(df[column])][column].min()\n",
    "        df.loc[df[column] == np.inf, column] = max_finite_value\n",
    "        df.loc[df[column] == -np.inf, column] = min_finite_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.query('Label == \"BENIGN\"').sample(frac=0.6, random_state=RANDOM_SEED)\n",
    "df_val_test = df.drop(df_train.index)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val_test = df_val_test.reset_index(drop=True)\n",
    "\n",
    "X_train = df_train.drop('Label', axis='columns')\n",
    "\n",
    "X_val, X_test, classes_val, classes_test = train_test_split(df_val_test.drop('Label', axis='columns'), df_val_test['Label'], test_size=0.65, stratify=df_val_test['Label'], random_state=RANDOM_SEED)\n",
    "\n",
    "X_val, X_test = X_val.reset_index(drop=True), X_test.reset_index(drop=True)\n",
    "classes_val, classes_test =  classes_val.reset_index(drop=True), classes_test.reset_index(drop=True)\n",
    "\n",
    "y_val, y_test = classes_val.apply(lambda c: 0 if c == 'BENIGN' else 1), classes_test.apply(lambda c: 0 if c == 'BENIGN' else 1)\n",
    "\n",
    "del df_train, df_val_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fwd PSH Flags',\n",
       " 'Fwd URG Flags',\n",
       " 'Fwd Packet Length Mean',\n",
       " 'Fwd Header Length',\n",
       " 'Total Fwd Packets',\n",
       " 'Total Length of Fwd Packets',\n",
       " 'Total Backward Packets',\n",
       " 'Bwd Packet Length Mean',\n",
       " 'Total Length of Bwd Packets',\n",
       " 'Subflow Fwd Packets',\n",
       " 'Flow Duration',\n",
       " 'Subflow Bwd Packets',\n",
       " 'RST Flag Count',\n",
       " 'Packet Length Mean',\n",
       " 'Flow IAT Max',\n",
       " 'Idle Mean',\n",
       " 'Fwd IAT Total',\n",
       " 'Max Packet Length',\n",
       " 'Fwd Packet Length Max',\n",
       " 'Bwd IAT Max',\n",
       " 'Bwd IAT Mean',\n",
       " 'Fwd IAT Max',\n",
       " 'Fwd IAT Mean',\n",
       " 'Idle Max']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_highly_correlated_features(correlation_matrix, threshold):\n",
    "    correlated_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                pair = (correlation_matrix.columns[i], correlation_matrix.columns[j])\n",
    "                coefficient = correlation_matrix.iloc[i, j]\n",
    "                correlated_pairs.append((pair, coefficient))\n",
    "    return sorted(correlated_pairs, key= lambda pair: pair[1], reverse=True)\n",
    "\n",
    "corr_matrix = X_train.corr().abs()\n",
    "correlation_list = get_highly_correlated_features(corr_matrix, 0.95)\n",
    "\n",
    "correlation_list[:10]\n",
    "\n",
    "# Drop high correlated features in correlation list\n",
    "\n",
    "f2drop = []\n",
    "for feature_pair, _ in correlation_list:\n",
    "    if feature_pair[0] not in f2drop and feature_pair[1] not in f2drop:\n",
    "        f2drop.append(feature_pair[1])\n",
    "\n",
    "f2drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2drop = f2drop + ['Destination Port']\n",
    "\n",
    "X_train = X_train.drop(f2drop, axis='columns')\n",
    "X_val = X_val.drop(f2drop, axis='columns')\n",
    "X_test = X_test.drop(f2drop, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaler = std_scaler.fit(X_train)\n",
    "\n",
    "norm_X_train = std_scaler.transform(X_train)\n",
    "norm_X_val = std_scaler.transform(X_val)\n",
    "norm_X_test = std_scaler.transform(X_test)\n",
    "\n",
    "# Salvando os dados processados em \"data_preprocessed\"\n",
    "X_train = pd.DataFrame(norm_X_train, columns=X_train.columns)\n",
    "X_val = pd.DataFrame(norm_X_val, columns=X_val.columns)\n",
    "X_test = pd.DataFrame(norm_X_test, columns=X_test.columns)\n",
    "\n",
    "# Salvando os arquivos processados\n",
    "\n",
    "RESULT_DIR = os.path.join(PROCESSED_DATA_DIR, DATASET_NAME)\n",
    "X_train.to_csv(os.path.join(RESULT_DIR, 'X_train.csv'), index=False)\n",
    "X_val.to_csv(os.path.join(RESULT_DIR, 'X_val.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(RESULT_DIR, 'X_test.csv'), index=False)\n",
    "y_val.to_csv(os.path.join(RESULT_DIR, 'y_val.csv'), index=False)\n",
    "y_test.to_csv(os.path.join(RESULT_DIR, 'y_test.csv'), index=False)\n",
    "\n",
    "del X_train, X_val, X_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
