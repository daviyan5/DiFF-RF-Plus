{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming RESULT_DIR is defined as in your original code\n",
    "# Definindo o BASE_DIR\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# Caminho para a pasta de dados\n",
    "\n",
    "RANDOM_SEED = 99\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "RESULT_DIR = os.path.join(BASE_DIR, 'results')\n",
    "\n",
    "# Function to safely parse dictionary-like strings\n",
    "def parse_dict_string(dict_str):\n",
    "    if not isinstance(dict_str, str):\n",
    "        return dict_str\n",
    "    \n",
    "    if not dict_str.startswith('{'):\n",
    "        return dict_str\n",
    "        \n",
    "    # Clean the string representation for proper parsing\n",
    "    try:\n",
    "        # Remove np.float64 references and parentheses\n",
    "        cleaned_str = dict_str.replace('np.float64', '').replace('(', '').replace(')', '')\n",
    "        # Parse the dictionary string\n",
    "        result_dict = ast.literal_eval(cleaned_str)\n",
    "        return result_dict\n",
    "    except (SyntaxError, ValueError) as e:\n",
    "        print(f\"Error parsing dictionary string: {e}\")\n",
    "        print(f\"Problematic string: {dict_str}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 'results.csv' not found in '/mnt/F02E8D3A2E8CFABC/SharedDocs/DETEC-INVASAO/Projeto/DiFF-RF-Plus/tests/results/donut_20250329_174820'\n"
     ]
    }
   ],
   "source": [
    "# Collect results from subfolders\n",
    "results_dfs = []\n",
    "for folder_name in os.listdir(RESULT_DIR):\n",
    "    folder_path = os.path.join(RESULT_DIR, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Extract dataset name from folder name\n",
    "        parts = folder_name.split('_')\n",
    "        dataset_name = parts[0]\n",
    "        for i in range(1, len(parts)-2):  # Skip the last two parts (date and id)\n",
    "            dataset_name += '_' + parts[i]\n",
    "            \n",
    "        results_csv_path = os.path.join(folder_path, 'results.csv')\n",
    "        if os.path.exists(results_csv_path):\n",
    "            try:\n",
    "                df = pd.read_csv(results_csv_path)\n",
    "                df['dataset_name'] = dataset_name\n",
    "                results_dfs.append(df)\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"Warning: 'results.csv' in '{folder_path}' is empty.\")\n",
    "            except pd.errors.ParserError:\n",
    "                print(f\"Warning: Could not parse 'results.csv' in '{folder_path}'. Check file format.\")\n",
    "        else:\n",
    "            print(f\"Warning: 'results.csv' not found in '{folder_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['BNA', 'collective', 0.9983606557377048, 1.0], ['CICIDS_2017', 'collective', 0.5249125, 0.2084031664716609], ['CIDDS-001_ICMP', 'collective', 0.9910161742542246, 0.9863013698630136], ['CIDDS-001_TCP', 'collective', 0.5, 0.0], ['CIDDS-001_UDP', 'collective', 0.5, 0.0], ['CTG', 'collective', 0.8788237407072438, 0.895397489539749], ['DCCC', 'collective', 0.6576493578141884, 0.7306924101198402], ['HTRU2', 'collective', 0.9402514395796796, 0.9435637285986048], ['Kitsune_Active_Wiretap', 'collective', 0.952575, 0.8911850331694745], ['Kitsune_ARP_MitM', 'collective', 0.8333825791779926, 0.552682513760358], ['Kitsune_Fuzzing', 'collective', 0.89705, 0.5521312565822294], ['Kitsune_Mirai', 'collective', 0.942695761100437, 0.99998507618607], ['Kitsune_OS_Scan', 'collective', 0.99181875, 0.9905068059671506], ['Kitsune_SSDP_Flood', 'collective', 0.999675, 0.9986015383078614], ['Kitsune_SSL_Renegotiation', 'collective', 0.97575625, 0.8426695934822069], ['Kitsune_SYN_DoS', 'collective', 0.9058531231702276, 0.0929364226893765], ['Kitsune_Video_Injection', 'collective', 0.7927249999999999, 0.5253742748966619], ['MAGIC', 'collective', 0.8138395723066002, 0.9285714285714286], ['MUSK', 'collective', 0.8204907794001526, 0.8926829268292683], ['Occupancy', 'collective', 0.9931504377642398, 0.9932843651626444], ['SPAM', 'collective', 0.8562547076371961, 0.9769553072625698], ['SPF', 'collective', 0.7563647318973686, 0.9143426294820716], ['UNSW_dns', 'collective', 0.999925, 1.0], ['UNSW_ftp', 'collective', 0.9915324804805776, 0.9516892958635932], ['UNSW_http', 'collective', 0.9930950801459572, 0.9853204251289066], ['UNSW_smtp', 'collective', 0.9998019385391322, 0.9993989180524944], ['UNSW_ssh', 'collective', 0.9707676764293808, 0.2465753424657534]]\n",
      "[['BNA', 'pointwise', 0.9525661630772528, 0.9833887043189368], ['CICIDS_2017', 'pointwise', 0.51915625, 0.2064353577240039], ['CIDDS-001_ICMP', 'pointwise', 0.8500639897616382, 0.796149490373726], ['CIDDS-001_TCP', 'pointwise', 0.5, 0.0], ['CIDDS-001_UDP', 'pointwise', 0.5, 0.0], ['CTG', 'pointwise', 0.7111404032045978, 0.7925], ['DCCC', 'pointwise', 0.6244059018585859, 0.6824106517168885], ['HTRU2', 'pointwise', 0.8301993910726173, 0.6839554047503635], ['Kitsune_Active_Wiretap', 'pointwise', 0.86375, 0.7100788781770376], ['Kitsune_ARP_MitM', 'pointwise', 0.7189062087165418, 0.375326585724636], ['Kitsune_Fuzzing', 'pointwise', 0.55220625, 0.2456547879317883], ['Kitsune_Mirai', 'pointwise', 0.9414095739634324, 0.9998952675913042], ['Kitsune_OS_Scan', 'pointwise', 0.99704375, 0.9937618524802876], ['Kitsune_SSDP_Flood', 'pointwise', 0.99660625, 0.9743214929591192], ['Kitsune_SSL_Renegotiation', 'pointwise', 0.91984375, 0.6574395641811372], ['Kitsune_SYN_DoS', 'pointwise', 0.6166132442087255, 0.0245063261203736], ['Kitsune_Video_Injection', 'pointwise', 0.75265625, 0.5959353574926542], ['MAGIC', 'pointwise', 0.7760702335420856, 0.9026288481494292], ['MUSK', 'pointwise', 0.7796404718707663, 0.8816326530612245], ['Occupancy', 'pointwise', 0.6645558773594328, 0.70868735911219], ['SPAM', 'pointwise', 0.6440675369246798, 0.8687552921253175], ['SPF', 'pointwise', 0.7079711247089657, 0.8654545454545455], ['UNSW_dns', 'pointwise', 0.999925, 1.0], ['UNSW_ftp', 'pointwise', 0.973675662202642, 0.9040635502597004], ['UNSW_http', 'pointwise', 0.9648101942140748, 0.919610598153352], ['UNSW_smtp', 'pointwise', 0.9988561424397258, 0.9936254980079682], ['UNSW_ssh', 'pointwise', 0.9432992280255204, 0.0304568527918781]]\n"
     ]
    }
   ],
   "source": [
    "# Process the data\n",
    "data_collective = []\n",
    "data_pointwise = []\n",
    "\n",
    "for df in results_dfs:\n",
    "    for index, row in df.iterrows():\n",
    "        dataset_name = row['dataset_name']\n",
    "        model_name = row.get('model', 'Unknown')  # Get model name or default to 'Unknown'\n",
    "        metric_type = row.get('metric_type', '')\n",
    "        \n",
    "        # Extract metrics directly from the row\n",
    "        auc = row.get('auc', None)\n",
    "        accuracy = row.get('accuracy', None)\n",
    "        tpr = row.get('true positive rate', None)\n",
    "        fpr = row.get('false positive rate', None)\n",
    "        precision = row.get('precision', None)\n",
    "        f1_score = row.get('f1-score', None)\n",
    "        \n",
    "        # Determine which list to append to based on metric_type\n",
    "        if metric_type == 'collective':\n",
    "            data_collective.append([\n",
    "                dataset_name,\n",
    "                metric_type,\n",
    "                auc,\n",
    "                precision\n",
    "            ])\n",
    "        else:  # Assuming anything that's not collective is pointwise\n",
    "            data_pointwise.append([\n",
    "                dataset_name,\n",
    "                metric_type,\n",
    "                auc,\n",
    "                precision,\n",
    "            ])\n",
    "print(data_collective)\n",
    "print(data_pointwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "df_collective = pd.DataFrame(data_collective, columns=['dataset_name', 'metric_type', 'auc', 'ap'])\n",
    "df_pointwise = pd.DataFrame(data_pointwise, columns=['dataset_name', 'metric_type', 'auc', 'ap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     dataset_name  auc_pointwise  ap_pointwise  auc_collective  ap_collective\n",
      "0             BNA       0.952566      0.983389        0.998361       1.000000\n",
      "1     CICIDS_2017       0.519156      0.206435        0.524913       0.208403\n",
      "2  CIDDS-001_ICMP       0.850064      0.796149        0.991016       0.986301\n",
      "3   CIDDS-001_TCP       0.500000      0.000000        0.500000       0.000000\n",
      "4   CIDDS-001_UDP       0.500000      0.000000        0.500000       0.000000\n"
     ]
    }
   ],
   "source": [
    "# Rename columns to indicate collective/pointwise\n",
    "df_collective = df_collective.rename(columns={\n",
    "    'auc': 'auc_collective',\n",
    "    'ap': 'ap_collective'\n",
    "})\n",
    "\n",
    "df_pointwise = df_pointwise.rename(columns={\n",
    "    'auc': 'auc_pointwise',\n",
    "    'ap': 'ap_pointwise'\n",
    "})\n",
    "\n",
    "# Merge the DataFrames on dataset_name\n",
    "df_merged = pd.merge(\n",
    "    df_pointwise[['dataset_name', 'auc_pointwise', 'ap_pointwise']], \n",
    "    df_collective[['dataset_name', 'auc_collective', 'ap_collective']], \n",
    "    on='dataset_name', \n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(df_merged.head())\n",
    "\n",
    "# Save to CSV\n",
    "csv_file_path = os.path.join(RESULT_DIR, 'merged_results.csv')\n",
    "df_merged.to_csv(csv_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
